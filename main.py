from fastapi import FastAPI
from pydantic import BaseModel
from transformers import AutoTokenizer
import onnxruntime
import numpy as np
import os
import base64
import zipfile

app = FastAPI()

@app.get("/")
def root():
    return {"message": "âœ… Domates API Ã§alÄ±ÅŸÄ±yor!"}

# Global deÄŸiÅŸkenler
tokenizer = None
session = None

# Dosya adlarÄ±
MODEL_B64_PATH = "bert_model_base64.txt"
MODEL_ZIP_PATH = "bert_model.zip"
MODEL_PATH = "bert_domates_model_quant.onnx"

@app.on_event("startup")
def startup_event():
    global tokenizer, session

    # Varsa eski model dosyasÄ±nÄ± sil
    if os.path.exists(MODEL_PATH):
        os.remove(MODEL_PATH)

    # âœ… Base64 -> zip dosyasÄ± oluÅŸtur
    print("ğŸ“¥ Base64 model decode ediliyor...")
    try:
        with open(MODEL_B64_PATH, "rb") as f:
            decoded = base64.b64decode(f.read())
        with open(MODEL_ZIP_PATH, "wb") as f:
            f.write(decoded)
        print("âœ… ZIP dosyasÄ± yazÄ±ldÄ±.")
    except Exception as e:
        print(f"âŒ Base64 decode hatasÄ±: {e}")
        return

    # âœ… Zip -> onnx dosyasÄ±nÄ± Ã§Ä±kar
    try:
        with zipfile.ZipFile(MODEL_ZIP_PATH, 'r') as zip_ref:
            zip_ref.extractall(".")
        print("âœ… .onnx model Ã§Ä±karÄ±ldÄ±.")
    except Exception as e:
        print(f"âŒ ZIP Ã§Ä±karma hatasÄ±: {e}")
        return

    # âœ… Tokenizer yÃ¼kle
    try:
        print("ğŸ”¤ Tokenizer yÃ¼kleniyor...")
        tokenizer = AutoTokenizer.from_pretrained("Kahsi13/DomatesRailway")
        print("âœ… Tokenizer yÃ¼klendi.")
    except Exception as e:
        print(f"âŒ Tokenizer yÃ¼klenemedi: {e}")
        return

    # âœ… ONNX modeli yÃ¼kle
    try:
        print("ğŸ“¦ ONNX modeli yÃ¼kleniyor...")
        session = onnxruntime.InferenceSession(MODEL_PATH)
        print("âœ… Model baÅŸarÄ±yla yÃ¼klendi.")
    except Exception as e:
        print(f"âŒ Model yÃ¼klenemedi: {e}")

# GiriÅŸ modeli
class InputText(BaseModel):
    text: str

# Tahmin endpoint'i
@app.post("/predict")
def predict(input: InputText):
    try:
        print("ğŸ§ª Gelen veri:", input)

        if tokenizer is None or session is None:
            return {"error": "â³ Model yÃ¼kleniyor, lÃ¼tfen birazdan tekrar deneyin."}

        encoding = tokenizer.encode_plus(
            input.text,
            padding="max_length",
            truncation=True,
            max_length=128,
            return_tensors="np"
        )

        input_ids = encoding["input_ids"].astype(np.int64)
        attention_mask = encoding["attention_mask"].astype(np.int64)

        ort_inputs = {
            "input_ids": input_ids,
            "attention_mask": attention_mask
        }

        ort_outs = session.run(None, ort_inputs)
        prediction = int(np.argmax(ort_outs[0]))

        return {"prediction": prediction}

    except Exception as e:
        return {"error": str(e)}
